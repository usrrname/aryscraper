# This is a basic workflow to help you get started with Actions
name: Scrape images and Publish on S3

# runs on push to main
on:
  workflow_dispatch: # can also be manually run

jobs:
  setup:
    environment: development
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.9]
    timeout-minutes: 30

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0

      - name: install python requirements
        uses: actions/setup-python@v3
        with:
          python-version: "3.9"
          cache: "pip"
          cache-dependency-path: "**/requirements.txt"
      - run: |
          pip install --upgrade pip
          pip install -r requirements.txt
          python3 -m pytest

      - name: execute main script # run .py to get the latest data
        env:
          SERP_API_KEY: ${{ secrets.SERP_API_KEY }}
        run: python3 -m main
        continue-on-error: true

      - name: Archive scraped data
        uses: actions/upload-artifact@v3
        with:
          name: data-set
          path: ./data-set/**
          retention-days: 30

  publish:
    runs-on: ubuntu-latest
    needs: setup
    if: ${{ always() && contains(join(needs.*.result, ','), 'success') }}
    steps:
      - uses: actions/checkout@v2
      - name: archive scraped data
        uses: actions/download-artifact@v3
        with:
          name: data-set
      - name: sync folder
        uses: keithweaver/aws-s3-github-action@main
        with:
          command: cp
          source: ./data-set/
          destination: ${{ secrets.S3_BUCKET }}/data-set
          aws_access_key_id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws_secret_access_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws_region: "canada-central-1"
          flags: --recursive --debug
